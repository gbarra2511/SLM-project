{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gbarra2511/SLM-project/blob/main/SLMproject.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kYanRuvpDbh1"
      },
      "source": [
        "#1: Starting to build de small language model\n",
        "This project is based in Dr Raj Dandekar(MIT PhD in Machine learning) teachings. Our goal is to build a production level SLM, but for instance I'm are developing and documenting the process of building an slm from scratch.\n",
        "\n",
        "STEPS:\n",
        "\n",
        "1.   Creating Dataset\n",
        "2.   Tokenizing the dataset\n",
        "3.   Creating input target pairs\n",
        "4.   Creating the SLM archtecture\n",
        "5.   Setting up for pre training\n",
        "6.   pre training\n",
        "7.   Inferences\n",
        "\n",
        "there are many processes included in these steps, this is only a resume."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IIDA_PAPDEM4"
      },
      "outputs": [],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NyLOTOFoK7_9"
      },
      "outputs": [],
      "source": [
        "!pip install -U datasets fsspec huggingface_hub\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gnfqA8hfH2Xi"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "ds = load_dataset(\"roneneldan/TinyStories\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2FS50_nLdqn"
      },
      "source": [
        "#tokenizing the dataset using BPE (byte pair encoding)\n",
        "\n",
        "Goal: tokenize the dataset and store it's ID's in a single .bin file\n",
        "\n",
        "\n",
        "*   Tokenize in tokenIDs\n",
        "*   Create a fille called \"traind.bin\" and \"validation.bin\" where it will be stored the tokenIDs\n",
        "*   make sure that the tokenIDs are stored on a disk, rather than on the RAM for efficient computation\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZqGOpDRZLZw1"
      },
      "outputs": [],
      "source": [
        "!pip install tokenizers\n",
        "import tiktoken\n",
        "import os\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "enc = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "# take a look at https://github.com/karpathy/nanoGPT/blob/data/openwebtext/prepare.py\n",
        "\n",
        "def process(example):\n",
        "    ids = enc.encode_ordinary(example['text']) #encode_originary ignores special tokens\n",
        "    out = {'ids': ids, 'len': len(ids)}\n",
        "    return out\n",
        "\n",
        "if not os.path.exists(\"train.bin\"):\n",
        "    tokenized = ds.map(process, remove_columns=['text'], desc=\"tokenizing the splits\", num_proc=8)\n",
        "    for split, dset in tokenized.items():\n",
        "      arr_len = np.sum(dset['len'], dtype=np.uint64)\n",
        "      filename = f'{split}.bin'\n",
        "      dtype = np.uint16 #can do since enc.max_token_value == 50256 is < 2**16\n",
        "      arr = np.memmap(filename, dtype=dtype, mode='w+', shape=(arr_len,))\n",
        "      total_batches = 1024\n",
        "\n",
        "      idx = 0\n",
        "      for batch_idx in tqdm(range(total_batches), desc =f'writing{filename}'):\n",
        "        #batch together samples for faster write\n",
        "        batch = dset.shard(num_shards=total_batches, index=batch_idx, contiguous=True).with_format('numpy')\n",
        "        arr_batch = np.concatenate(batch['ids'])\n",
        "        #Write to mmap\n",
        "        arr[idx : idx + len(arr_batch)] = arr_batch\n",
        "        idx += len(arr_batch)\n",
        "      arr.flush()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ryxM4-5-aZ78"
      },
      "source": [
        "# **STEP 3** creating our input- output pairs\n",
        "\n",
        "For doing that we need to determine our context_size( lenght of words that my language model is looking at one time before predicting the next token) -- divide the text into chunks of words and the maximum number of words in the chunks is the context_size\n",
        "\n",
        "we need to determine thought our batch_size( we dont process all data at once, so we first take the output of the first batch, calculate the loss, propagate it backwards and updare the parameters, than do it again for all batches)\n",
        "\n",
        "#note: the output is going to be the input shifted to the right:\n",
        "example: one day a little girl named lily(phrase)\n",
        "input: one day a little girl named\n",
        "ouput: day a little girl named lily\n",
        "\n",
        "\n",
        "its trying to predict the next word (Lily), the context_size in this case is 6.The target is Lily, but the output can be different and thats why we calculate the loss, to try to minimize it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wsgWCM7EVOwZ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import math\n",
        "from dataclasses import dataclass\n",
        "import numpy as np\n",
        "from contextlib import nullcontext\n",
        "import os\n",
        "\n",
        " #non_blocking=TRUE allows the cpu to do other stuff while the process is going on\n",
        "\n",
        "\n",
        "# Some functions from https://github.com/karpathy/nanoGPT/blob/master/train.py with slight modifications\n",
        "#whenever get_batch is defined, we can create input  baches and output batches by moving right by 1, every get_batch function return an input and an output matriz.\n",
        "def get_batch(split):\n",
        "    # We recreate np.memmap every batch to avoid a memory leak, as per\n",
        "    # https://stackoverflow.com/questions/45132940/numpy-memmap-memory-usage-want-to-iterate-once/61472122#61472122\n",
        "    if split == 'train':\n",
        "        data = np.memmap('train.bin', dtype=np.uint16, mode='r')\n",
        "    else:\n",
        "        data = np.memmap('validation.bin', dtype=np.uint16, mode='r')\n",
        "    #here we take four random int that correspond to four different positions because batch_size == 4\n",
        "    #block_size is the context_size that is mentioned before\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    #x and y are stacking together the vectors x1, x2, x3, x4 , y1,...y4\n",
        "    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n",
        "    #see that y is only x shifted to ridght by 1,\n",
        "    y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n",
        "\n",
        "\n",
        "    #NOTE: CUDA is a platform and software created by nvidia that allows GPU's to process general purpose stuff and not only graphics\n",
        "    if device_type == 'cuda':\n",
        "        # pin arrays x,y locks the memory of the tensor in RAM and this allows faster transfer of tensor to GPU, because it reserves some memory in RAM.\n",
        "        #non_blocking=TRUE allows the cpu to do other stuff while the process is going on\n",
        "        x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n",
        "    else:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXa2syQPvyAD"
      },
      "source": [
        "# **step 4** - defing the SLM Model Architecture\n",
        "\n",
        "there are multiple different things happenning:\n",
        "\n",
        "longest code cell\n",
        "most important part\n",
        "\n",
        "\n",
        "1.   input block\n",
        "2.   processor block\n",
        "3.   output block\n",
        "\n",
        "#At the end we are feeding a sequence of words and we are trying to predict the next word or token\n",
        "\n",
        "# **Input Block**:\n",
        "**token embedding**: we convert every TokenID's into an multidimensional vector, words have sort of an semmantic notion which is very important to the language model to capture. Vectors next to each other will be similar in meaning. We maintaing a token embedding matriz that the column size is the vocabulary_size, example, english is 50000 words, and each TokenID is encoded as an high dimensional vector, the dimension of the token(embedding dimension) is choosen by me, and its maintenned throught the Slm architechture. The token embedding matrix serves as a lookup table, so when we have TokenID's we can look at the matrix and retrieve the corresponding vector.\n",
        "\n",
        "#**Transformer block**\n",
        "**layer normalization**: first step to prevent different distribution by back propagation, so its stablishes a range(internal coehrent shift) to improve training performance. PyTorch( layer_norm ), normalizes rows\n",
        "\n",
        "**Multi-head attention**:\n",
        "in the following phrases \"The dog chased the ball. It could not catch it.\"\n",
        "\n",
        "the first it is reffering to the dog, the second one is reffering to the ball, but how the model can differentiate, and stabilishes relations between words?\n",
        "\n",
        "Attention mechanism **augments the inputs embedding vector** so for every vector it can have information of the neighboors. We capture, basically, the attention scores between the token and all the tokens arround it. We want to transform the normal vector into a context vector, because it is much richer in information than the other, it has the information about how the word relates to all the other words arround it.\n",
        "\n",
        "quick and superficial recap of the process, we have the matrix called query, key and values. The called atttention weight matrix is given by doing dot product(escalar) between query and key, the coordinate stabilishes the relation between the Xn and Yn values, where Xn and Yn are words, for example: point A12 = 0,7 where x1 is One and Y2 is day, so 0,7 is the atttention weight between this words. Multiplying this matrix with the value matrix is given the context matrix.\n",
        "\n",
        "**Dropout**\n",
        "neurons ar turned-off randomly during training to **not overfit the model** and guarantee that the model is really learning. How it occurs? the output of some neurons are turned to zero\n",
        "\n",
        "**Feed Forward Neural Network**\n",
        "without this the model cannot leran the patterns in the underlying data. Every time in deep learning if a model is not working in a small dimensional space, we project it to an high dimensional space so it can capture and understand an non-linearities. So we exppand the model in the intermidiate layers than compress for the output, so it can have the same size as the input layer.\n",
        "The activation function here is GeLU. Hyperparameter, very good results.\n",
        "\n",
        "**Shortcuts**\n",
        "The input of the first layer is added to the output of the first layer, and it turns into the input of the next layer.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5SEStGVJzaMC"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "from dataclasses import dataclass\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "from contextlib import nullcontext\n",
        "import os\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, ndim, bias):\n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter(torch.ones(ndim))\n",
        "        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n",
        "    def forward(self, x):\n",
        "        return F.layer_norm(x, self.weight.shape, self.weight, self.bias, 1e-5)\n",
        "\n",
        "class CausalSelfAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.n_embd % config.n_head == 0\n",
        "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
        "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
        "        self.attn_dropout = nn.Dropout(config.dropout)\n",
        "        self.resid_dropout = nn.Dropout(config.dropout)\n",
        "        self.n_head = config.n_head\n",
        "        self.n_embd = config.n_embd\n",
        "        self.flash = hasattr(F, 'scaled_dot_product_attention')\n",
        "        if not self.flash:\n",
        "            self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
        "                                       .view(1, 1, config.block_size, config.block_size))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size()\n",
        "        q, k, v = self.c_attn(x).split(self.n_embd, dim=2)\n",
        "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "\n",
        "        if self.flash:\n",
        "            y = F.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.attn_dropout.p if self.training else 0.0, is_causal=True)\n",
        "        else:\n",
        "            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
        "            att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float('-inf'))\n",
        "            att = F.softmax(att, dim=-1)\n",
        "            att = self.attn_dropout(att)\n",
        "            y = att @ v\n",
        "\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
        "        y = self.resid_dropout(self.c_proj(y))\n",
        "        return y\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n",
        "        self.gelu = nn.GELU()\n",
        "        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "    def forward(self, x):\n",
        "        return self.dropout(self.c_proj(self.gelu(self.c_fc(x))))\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln1 = LayerNorm(config.n_embd, config.bias)\n",
        "        self.attn = CausalSelfAttention(config)\n",
        "        self.ln2 = LayerNorm(config.n_embd, config.bias)\n",
        "        self.mlp = MLP(config)\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln1(x))\n",
        "        x = x + self.mlp(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "@dataclass\n",
        "class GPTConfig:\n",
        "    block_size: int\n",
        "    vocab_size: int\n",
        "    n_layer: int\n",
        "    n_head: int\n",
        "    n_embd: int\n",
        "    dropout: float = 0.0\n",
        "    bias: bool = True\n",
        "#class gpt is the complete model where its taking the blocks and the embedding and\n",
        "class GPT(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.transformer = nn.ModuleDict(dict(\n",
        "            #wte = word token embedding-> convert words tokens into vectors\n",
        "            wte=nn.Embedding(config.vocab_size, config.n_embd),\n",
        "            #wpe = word position embedding-> creates a vector for every word position, putting like an order\n",
        "            wpe=nn.Embedding(config.block_size, config.n_embd),\n",
        "            drop=nn.Dropout(config.dropout),\n",
        "            h=nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
        "            ln_f=LayerNorm(config.n_embd, config.bias),\n",
        "        ))\n",
        "        #projects the transformer output in the size of vocabulary creating the logits\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "        # weight tying-> the embedding layer and the output layer has the same weights(better efficiency and reduces parameters)\n",
        "        self.transformer.wte.weight = self.lm_head.weight\n",
        "\n",
        "        self.apply(self._init_weights)\n",
        "        for pn, p in self.named_parameters():\n",
        "            if pn.endswith('c_proj.weight'):\n",
        "                nn.init.normal_(p, mean=0.0, std=0.02 / math.sqrt(2 * config.n_layer))\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    #here its being defined the foward pass\n",
        "    def forward(self, idx, targets=None):\n",
        "        #the token id(idx) are converted into vectors by wte and wpe, they are being added together.\n",
        "\n",
        "        device = idx.device\n",
        "        b, t = idx.size()\n",
        "        assert t <= self.config.block_size\n",
        "        pos = torch.arange(0, t, dtype=torch.long, device=device)\n",
        "\n",
        "        tok_emb = self.transformer.wte(idx)\n",
        "        pos_emb = self.transformer.wpe(pos)\n",
        "\n",
        "        x = self.transformer.drop(tok_emb + pos_emb)\n",
        "        #here the result x is passing in every block and than is normalized\n",
        "        for block in self.transformer.h:\n",
        "            x = block(x)\n",
        "        x = self.transformer.ln_f(x)\n",
        "        #if targets are being\n",
        "        if targets is not None:\n",
        "            logits = self.lm_head(x)\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
        "            return logits, loss\n",
        "        else:\n",
        "            logits = self.lm_head(x[:, [-1], :])\n",
        "            return logits, None\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
        "        \"\"\"\n",
        "        Generate tokens given a conditioning sequence.\n",
        "        idx: Tensor of shape (B, T)\n",
        "        \"\"\"\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n",
        "            logits, _ = self(idx_cond)\n",
        "            logits = logits[:, -1, :] / temperature\n",
        "            if top_k is not None:\n",
        "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
        "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "        return idx"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#OUTPUT\n",
        "\n",
        "there is other normilazation. The output head it is another neural network that convert every vector into vocab_size. After passing the output N.N it is formed the logits matrix or tensor, where every element corresponds to the probability of that line(id) being the next token.\n",
        "\n",
        "**How it is used?And why?**\n",
        "\n",
        "in the batch logits tensor it is applied softmax function to give the probabilities, them we take the highest value of each line(highest probability- output), where each line is an word, then calculates the loss betwween this output and the given target. This loss is used to correct the weights in backpropagation. In the training routine we need to minimize this loss function using cross entropy."
      ],
      "metadata": {
        "id": "GJUEpYSTHIh0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dYnTyyeQjqmB"
      },
      "outputs": [],
      "source": [
        "config = GPTConfig(\n",
        "    vocab_size=50257,     # use the tokenizer's vocab size\n",
        "    block_size=128,       # or whatever context size you're training with\n",
        "    n_layer=6,\n",
        "    n_head=6,\n",
        "    n_embd=384,\n",
        "    dropout=0.1,\n",
        "    bias=True\n",
        ")\n",
        "\n",
        "model = GPT(config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jktpL1Xp_KXq"
      },
      "source": [
        "#defining the loss function\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-20I33pNkqm4"
      },
      "outputs": [],
      "source": [
        "def estimate_loss(model):\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    with torch.inference_mode():\n",
        "        for split in ['train', 'val']:\n",
        "            losses = torch.zeros(eval_iters)\n",
        "            for k in range(eval_iters):\n",
        "                X, Y = get_batch(split)\n",
        "                with ctx:\n",
        "                    logits, loss = model(X, Y)\n",
        "                losses[k] = loss.item()\n",
        "            out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MvDFYp5YlYfj"
      },
      "outputs": [],
      "source": [
        "# Training Config\n",
        "import torch\n",
        "from contextlib import nullcontext\n",
        "\n",
        "learning_rate = 1e-4 #more stable training, earlier 1e-4\n",
        "max_iters = 20000 #increase from 25000\n",
        "warmup_steps = 1000 #smoother initial train, earlier 100\n",
        "min_lr = 5e-4 #lower rate, earlier 5e-4\n",
        "eval_iters = 500 # increased from 100\n",
        "batch_size = 32 # changed from 16, better gradient estimate\n",
        "block_size = 128 #changed from 64, capture longer range dependencies\n",
        "\n",
        "gradient_accumulation_steps = 32 # reduced from 50\n",
        "\n",
        "device =  \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n",
        "# note: float16 data type will automatically use a GradScaler\n",
        "\n",
        "#dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' # 'float32', 'bfloat16', or 'float16', the latter will auto implement a GradScaler\n",
        "dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' # 'float32', 'bfloat16', or 'float16', the latter will auto implement a GradScaler\n",
        "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
        "#amp is automatic mixed precision, it enables to use float16 when its safe and float32 when its needed more precision(MATRIX MULTIPLICATIONS AND DROPOUT GELU)\n",
        "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n",
        "\n",
        "torch.set_default_device(device)\n",
        "torch.manual_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BDvcEZi-amJm"
      },
      "outputs": [],
      "source": [
        "from torch.optim.lr_scheduler import LinearLR,SequentialLR, CosineAnnealingLR\n",
        "\n",
        "##PUT IN WEIGHT DECAY, CHANGED BETA2 to 0.95\n",
        "#THE ERROR IS BEING AJUSTED(FINAL STEP OF BACKPROPAGATION)\n",
        "optimizer =  torch.optim.AdamW(model.parameters(), lr=learning_rate, betas=(0.9, 0.95), weight_decay=0.1, eps=1e-9) #weight decay for regularization\n",
        "\n",
        "scheduler_warmup = LinearLR(optimizer, total_iters = warmup_steps) #Implement linear warmup\n",
        "scheduler_decay = CosineAnnealingLR(optimizer,T_max = max_iters - warmup_steps, eta_min = min_lr) #Implement lr decay\n",
        "scheduler = SequentialLR(optimizer, schedulers=[scheduler_warmup, scheduler_decay], milestones=[warmup_steps]) #Switching from warmup to decay\n",
        "\n",
        "# https://stackoverflow.com/questions/72534859/is-gradscaler-necessary-with-mixed-precision-training-with-pytorch\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training loop\n",
        "\n",
        "**explaining back propagation**:\n",
        "is the process which neural networks learns from their mistakes. Allows you to know exactly the steepness of the gradient for every setting, so you know precisely how to adjust the weights for a better perfomance.\n",
        "\n",
        "There are four main steps:\n",
        "\n",
        "1.   Forward pass\n",
        "2.   Calculating error\n",
        "3.   Backward Pass\n",
        "4.   Adjusting error\n",
        "\n",
        "At the final layer of the network it takes an guess, calculate the error using some function, in this case softmax, then move backwards layer by layer calculating with the chain rule(CALCULUS) how much each knob contributed for the result. If a knob is steep, it will have a big impact on the final result, it is more important. If a knob is shallow, it has less importance for the final result.\n",
        "\n",
        "When the backward pass gets in the first layer, there is a gradient assigned to every knob(adjustable parameter). The next step is adjust for a better performance.\n",
        "\n",
        "Updating weights: changes the weight in the opposite direction(gradient descent) using the learning rate as parameter. For example: if knobX weight increases, the learning rate decreases, so after the process it will adjust knobX weight lower so the learning rate can get higher.\n",
        "\n",
        "After thousands or millions of times the network gets better.\n",
        "\n"
      ],
      "metadata": {
        "id": "3kbXoNS_DGhv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H38gz5DgcWy4"
      },
      "outputs": [],
      "source": [
        "best_val_loss = float('inf')\n",
        "best_model_params_path = \"best_model_params.pt\"\n",
        "train_loss_list, validation_loss_list = [], []\n",
        "\n",
        "# Ensure model is on the correct device\n",
        "model = model.to(device)\n",
        "\n",
        "# In your training loop\n",
        "for epoch in tqdm(range(max_iters)):\n",
        "    if epoch % eval_iters == 0 and epoch != 0:\n",
        "        # Ensure estimate_loss uses the correct device\n",
        "        losses = estimate_loss(model)\n",
        "        print(f\"Epoch {epoch}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "        print(f\"The current learning rate: {optimizer.param_groups[0]['lr']:.5f}\")\n",
        "        train_loss_list += [losses['train']]\n",
        "        validation_loss_list += [losses['val']]\n",
        "\n",
        "        if losses['val'] < best_val_loss:\n",
        "            best_val_loss = losses['val']\n",
        "            torch.save(model.state_dict(), best_model_params_path)\n",
        "\n",
        "    # Ensure X and y are on the correct device\n",
        "    X, y = get_batch(\"train\")\n",
        "    X, y = X.to(device), y.to(device)\n",
        "\n",
        "    with ctx:\n",
        "        logits, loss = model(X, y)\n",
        "        loss = loss / gradient_accumulation_steps\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "    if ((epoch + 1) % gradient_accumulation_steps == 0) or (epoch + 1 == max_iters):\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "    scheduler.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fFY62Apmcljr"
      },
      "outputs": [],
      "source": [
        "import matplotlib as plt\n",
        "train_loss_list_converted = [i.cpu().detach() for i in train_loss_list]\n",
        "validation_loss_list_converted = [i.cpu().detach() for i in validation_loss_list]\n",
        "\n",
        "plt.plot(train_loss_list_converted, 'g', label = 'train_loss')\n",
        "plt.plot(validation_loss_list_converted, 'r', label = 'validation_loss')\n",
        "plt.xlabel(\"Steps- Every 100 epochs\")\n",
        "plt.ylabel(\"loss\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rt_K4_Tefp9d"
      },
      "outputs": [],
      "source": [
        "#loading the model\n",
        "model = GPT(config)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "best_model_params_path = \"best_model_params.pt\"\n",
        "model.load_state_dict(torch.load(best_model_params_path, map_location=torch.device(device)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uUwL-ETegYL4"
      },
      "outputs": [],
      "source": [
        "sentence = \"Once upon a time there was a pumpkin\"\n",
        "context = (torch.tensor(enc.encode_ordinary(sentence).unsqueeze(dim = 0)))\n",
        "y = model.generate(context, 200)\n",
        "print(enc.decode(y.squeeze().tolist()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G4zwcaMAg5uQ"
      },
      "outputs": [],
      "source": [
        "sentence = \"A little girl went to the woods\"\n",
        "context = (torch.tensor(enc.encode_ordinary(sentence).unsqueeze(dim = 0)))\n",
        "y = model.generate(context, 200)\n",
        "print(enc.decode(y.squeeze().tolist()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gJz0bqWOhFP9"
      },
      "outputs": [],
      "source": [
        "from google.colab import runtime\n",
        "runtime.unassign"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyO5ncata6bguV2kOzYVNM6+",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}